{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["74nTZ9coNflk","KT2Gg_v23Md_","DKV9JYfgzeWN","NZTsUW2yLKSI"],"authorship_tag":"ABX9TyNN+6KEJYi3bZZlvrcCs/w3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**CHAP 8 : Generative AI for Data Preparation**"],"metadata":{"id":"L0ZxV48JXZTm"}},{"cell_type":"code","source":["GEMINI_KEY = \"[PUT YOUR GEMEINI KEY HERE]\""],"metadata":{"id":"RJ2iHrudl1Y_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prompt template"],"metadata":{"id":"74nTZ9coNflk"}},{"cell_type":"code","source":["# Define a simple function to create a prompt for an LLM\n","def create_prompt(task, details):\n","    # Template for the prompt\n","    prompt = f\"Task: {task}\\nDetails: {details}\\nResponse:\"\n","    return prompt\n","\n","# Example usage\n","task = \"Explain the difference between supervised and unsupervised learning.\"\n","details = \"Supervised learning uses labeled data to train models, while unsupervised learning deals with unlabeled data.\"\n","\n","# Generate the prompt\n","prompt = create_prompt(task, details)\n","\n","# Print the generated prompt\n","print(\"Generated Prompt:\")\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61T9IDPcuoO4","executionInfo":{"status":"ok","timestamp":1733324402950,"user_tz":-60,"elapsed":8,"user":{"displayName":"Benoît Cayla","userId":"16127805405482571098"}},"outputId":"f94b809b-f7ee-42a9-ab74-346be7291324"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated Prompt:\n","Task: Explain the difference between supervised and unsupervised learning.\n","Details: Supervised learning uses labeled data to train models, while unsupervised learning deals with unlabeled data.\n","Response:\n"]}]},{"cell_type":"markdown","source":["chatGPT's answer:  \n","\n","Supervised and unsupervised learning are two main types of machine learning, distinguished by the presence or absence of labeled data during the training process.\n","\n","    Supervised Learning: This approach uses labeled data, meaning that each input is paired with a known output (or label). The model learns to map inputs to the correct outputs by minimizing the error between predictions and the actual labels. Common tasks include classification (e.g., email spam detection) and regression (e.g., predicting house prices).\n","\n","    Unsupervised Learning: In this approach, the data is unlabeled, and the model tries to uncover patterns or structures within the data without explicit guidance. It focuses on finding similarities, differences, or clusters in the data. Common tasks include clustering (e.g., customer segmentation) and dimensionality reduction (e.g., principal component analysis for data compression).\n","\n","In essence, supervised learning is about predicting outcomes, while unsupervised learning is about discovering hidden patterns."],"metadata":{"id":"CicbE5SHN1qT"}},{"cell_type":"markdown","source":["# By using Gemini"],"metadata":{"id":"dHqA_lcw8FZ9"}},{"cell_type":"markdown","source":["Gemini provides a free API tier with lower rate limits for testing purposes. You can create a free API key through [Google AI Studio](https://aistudio.google.com/prompts/new_chat) to access this tier.\n","\n","However, it's important to note that there are usage limits and potential costs associated with exceeding these limits. For production use cases, it's recommended to review the pricing details and consider paid plans to ensure optimal performance and cost-effectiveness.\n","\n","Configuring and Using the Gemini API:\n","1. Create a Google AI Studio Account: If you don't have one already, sign up for a free account.\n","2. Generate an API Key: Once logged in, navigate to the API Key section and create a new key.\n","3. Store Your API Key Securely: Keep your API key confidential. Avoid sharing it publicly.\n","\n","Note: the currentfree tier limitations and may change in the future. You can find the most up-to-date information on the Gemini API pricing page: https://ai.google.dev/pricing"],"metadata":{"id":"eaRq6hAu8-1q"}},{"cell_type":"code","source":["import google.generativeai as genai\n","model = genai.GenerativeModel(\"gemini-pro\") # or gemini-1.5-flash-002 or gemini-1.5-flash-8b"],"metadata":{"id":"GP6YTfGb8U3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Simple prompt"],"metadata":{"id":"QMeFOzMSMWPn"}},{"cell_type":"code","source":["original = \"My customer is not satisfied\"\n","response = model.generate_content(f\"Paraphrase this in english: {original}\")\n","print(response.text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GEGpiLeD8ceX","executionInfo":{"status":"ok","timestamp":1733308385284,"user_tz":-60,"elapsed":5332,"user":{"displayName":"Benoît Cayla","userId":"16127805405482571098"}},"outputId":"ba762902-8d0b-4ed7-fd38-2fbadd9ad654"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["My client is unhappy\n"]}]},{"cell_type":"markdown","source":["# Using Generative AI to impute missing values"],"metadata":{"id":"ATNnpIDDmH5M"}},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/datacorner/dataprep-handbook/main/Titanic%20disaster/train.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LdLk3E0Vmb17","executionInfo":{"status":"ok","timestamp":1733324441882,"user_tz":-60,"elapsed":887,"user":{"displayName":"Benoît Cayla","userId":"16127805405482571098"}},"outputId":"335191a6-17e8-4c20-ec26-096ec9196b32"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-12-04 15:00:31--  https://raw.githubusercontent.com/datacorner/dataprep-handbook/main/Titanic%20disaster/train.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 60302 (59K) [text/plain]\n","Saving to: ‘train.csv’\n","\n","\rtrain.csv             0%[                    ]       0  --.-KB/s               \rtrain.csv           100%[===================>]  58.89K  --.-KB/s    in 0.002s  \n","\n","2024-12-04 15:00:31 (28.6 MB/s) - ‘train.csv’ saved [60302/60302]\n","\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv(\"/content/train.csv\")"],"metadata":{"id":"WHTGRo3amfgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Step 1: Split the dataset into manageable chunks\n","chunk_size = 5\n","chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n","\n","# Step 2: Collect metadata for each chunk\n","def collect_metadata(chunk):\n","    metadata = {\n","        \"columns\": chunk.columns.tolist(),\n","        \"missing_counts\": chunk.isna().sum().to_dict(),\n","        \"column_means\": chunk.mean(numeric_only=True).to_dict(),\n","        \"column_medians\": chunk.median(numeric_only=True).to_dict(),\n","        \"column_modes\": chunk.mode(dropna=True).iloc[0].to_dict()\n","    }\n","    return metadata\n","\n","# Step 3: Build the prompt for an LLM\n","def build_prompt(chunk, metadata):\n","    prompt = f\"\"\"\n","You are a data expert helping to clean a dataset. The dataset is structured as follows:\n","Columns: {metadata['columns']}\n","Metadata about the columns:\n","- Missing value counts: {metadata['missing_counts']}\n","- Column means: {metadata['column_means']}\n","- Column medians: {metadata['column_medians']}\n","- Column modes: {metadata['column_modes']}\n","\n","Here is the dataset with missing values (NaN):\n","{chunk.to_string(index=False)}\n","\n","Your task:\n","1. Identify columns with missing values.\n","2. Impute missing values using the most appropriate statistical method based on metadata (e.g., mean, median, or mode).\n","3. Return the updated data for the dataset provided in a JSON format (each row as a node).\n","Please only provide the result of task 3\n","\"\"\"\n","    return prompt\n","\n","\n","# Step 4: Generate prompts for each chunk\n","prompts = []\n","for chunk in chunks:\n","    metadata = collect_metadata(chunk)\n","    prompt = build_prompt(chunk, metadata)\n","    prompts.append(prompt)\n","\n","# Step 5: Example of sending a prompt to an LLM with Gemini. This step requires an LLM endpoint and API key.\n","def send_to_llm(prompt):\n","    # Replace with actual LLM API code (e.g., OpenAI GPT)\n","    response = model.generate_content(prompt)\n","    return response.text\n","\n","# Process each prompt with the LLM\n","#for i, prompt in enumerate(prompts):\n","#    print(f\"Processing Chunk {i+1}/{len(prompts)}\")\n","#    response = send_to_llm(prompt)\n","#    print(f\"Response for Chunk {i+1}:\\n{response}\\n\")\n","\n","print(f\"Prompt {prompts[0]}\")\n","response = send_to_llm(prompt)\n","print(f\"Response for Chunk {0}:\\n{response}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":770},"id":"bd1LNMA5m5km","executionInfo":{"status":"ok","timestamp":1733324627875,"user_tz":-60,"elapsed":8001,"user":{"displayName":"Benoît Cayla","userId":"16127805405482571098"}},"outputId":"19b38296-73cf-48a1-d968-9956865eea22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt \n","You are a data expert helping to clean a dataset. The dataset is structured as follows:\n","Columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n","Metadata about the columns:\n","- Missing value counts: {'PassengerId': 0, 'Survived': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Age': 0, 'SibSp': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 0, 'Cabin': 3, 'Embarked': 0}\n","- Column means: {'PassengerId': 3.0, 'Survived': 0.6, 'Pclass': 2.2, 'Age': 31.2, 'SibSp': 0.6, 'Parch': 0.0, 'Fare': 29.521660000000004}\n","- Column medians: {'PassengerId': 3.0, 'Survived': 1.0, 'Pclass': 3.0, 'Age': 35.0, 'SibSp': 1.0, 'Parch': 0.0, 'Fare': 8.05}\n","- Column modes: {'PassengerId': 1, 'Survived': 1.0, 'Pclass': 3.0, 'Name': 'Allen, Mr. William Henry', 'Sex': 'female', 'Age': 35.0, 'SibSp': 1.0, 'Parch': 0.0, 'Ticket': '113803', 'Fare': 7.25, 'Cabin': 'C123', 'Embarked': 'S'}\n","\n","Here is the dataset with missing values (NaN):\n"," PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Cabin Embarked\n","           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500   NaN        S\n","           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833   C85        C\n","           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250   NaN        S\n","           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000  C123        S\n","           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500   NaN        S\n","\n","Your task:\n","1. Identify columns with missing values.\n","2. Impute missing values using the most appropriate statistical method based on metadata (e.g., mean, median, or mode).\n","3. Return the updated data for the dataset provided in a JSON format (each row as a node).\n","Please only provide the result of task 3\n","\n","Response for Chunk 0:\n","[\n","  {\n","    \"PassengerId\": 891,\n","    \"Survived\": 0,\n","    \"Pclass\": 3,\n","    \"Name\": \"Dooley, Mr. Patrick\",\n","    \"Sex\": \"male\",\n","    \"Age\": 32.0,\n","    \"SibSp\": 0,\n","    \"Parch\": 0,\n","    \"Ticket\": \"370376\",\n","    \"Fare\": 7.75,\n","    \"Cabin\": \"NaN\",\n","    \"Embarked\": \"Q\"\n","  }\n","]\n","\n"]}]},{"cell_type":"markdown","source":["# Noise reduction and data normalization"],"metadata":{"id":"KT2Gg_v23Md_"}},{"cell_type":"code","source":["def build_normalization_prompt(chunk, metadata):\n","    prompt = f\"\"\"\n","You are a data expert helping to clean a dataset by performing noise reduction and normalization. The dataset is structured as follows:\n","Columns: {metadata['columns']}\n","Metadata about the columns:\n","- Missing value counts: {metadata['missing_counts']}\n","- Column means: {metadata['column_means']}\n","- Column medians: {metadata['column_medians']}\n","- Column modes: {metadata['column_modes']}\n","\n","Here is a sample of the data (including some noisy values or outliers) for analysis:\n","{chunk.head(5).to_string(index=False)}\n","\n","### Your task:\n","1. **Noise Reduction**:\n","   - Identify any columns that have noisy data (e.g., outliers, spikes, extreme values).\n","   - Apply an appropriate technique to reduce noise, such as:\n","     - Using a rolling average or median for smoothing.\n","     - Identifying and handling outliers (e.g., via z-scores or IQR).\n","     - Handling missing values (e.g., forward filling or imputation with statistical values).\n","\n","2. **Normalization**:\n","   - Normalize numerical features to ensure they are on a similar scale. You can apply one or more of the following methods:\n","     - **Min-Max Scaling**: Scale values between 0 and 1.\n","     - **Standardization (Z-Score Normalization)**: Adjust the dataset to have a mean of 0 and a standard deviation of 1.\n","     - **Robust Scaling**: Normalize using median and IQR to handle outliers.\n","\n","   Provide the updated values after applying these techniques.\n","\"\"\"\n","    return prompt\n","metadata = collect_metadata(chunk)\n","print(build_normalization_prompt(chunks[0], metadata))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HD-sAXTM3JXb","executionInfo":{"status":"ok","timestamp":1733324456810,"user_tz":-60,"elapsed":471,"user":{"displayName":"Benoît Cayla","userId":"16127805405482571098"}},"outputId":"0a23ad05-b04f-4078-9e0a-c9bed6795ad7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","You are a data expert helping to clean a dataset by performing noise reduction and normalization. The dataset is structured as follows:\n","Columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n","Metadata about the columns:\n","- Missing value counts: {'PassengerId': 0, 'Survived': 0, 'Pclass': 0, 'Name': 0, 'Sex': 0, 'Age': 0, 'SibSp': 0, 'Parch': 0, 'Ticket': 0, 'Fare': 0, 'Cabin': 1, 'Embarked': 0}\n","- Column means: {'PassengerId': 891.0, 'Survived': 0.0, 'Pclass': 3.0, 'Age': 32.0, 'SibSp': 0.0, 'Parch': 0.0, 'Fare': 7.75}\n","- Column medians: {'PassengerId': 891.0, 'Survived': 0.0, 'Pclass': 3.0, 'Age': 32.0, 'SibSp': 0.0, 'Parch': 0.0, 'Fare': 7.75}\n","- Column modes: {'PassengerId': 891, 'Survived': 0, 'Pclass': 3, 'Name': 'Dooley, Mr. Patrick', 'Sex': 'male', 'Age': 32.0, 'SibSp': 0, 'Parch': 0, 'Ticket': '370376', 'Fare': 7.75, 'Cabin': nan, 'Embarked': 'Q'}\n","\n","Here is a sample of the data (including some noisy values or outliers) for analysis:\n"," PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Cabin Embarked\n","           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500   NaN        S\n","           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833   C85        C\n","           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250   NaN        S\n","           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000  C123        S\n","           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500   NaN        S\n","\n","### Your task:\n","1. **Noise Reduction**: \n","   - Identify any columns that have noisy data (e.g., outliers, spikes, extreme values).\n","   - Apply an appropriate technique to reduce noise, such as:\n","     - Using a rolling average or median for smoothing.\n","     - Identifying and handling outliers (e.g., via z-scores or IQR).\n","     - Handling missing values (e.g., forward filling or imputation with statistical values).\n","\n","2. **Normalization**: \n","   - Normalize numerical features to ensure they are on a similar scale. You can apply one or more of the following methods:\n","     - **Min-Max Scaling**: Scale values between 0 and 1.\n","     - **Standardization (Z-Score Normalization)**: Adjust the dataset to have a mean of 0 and a standard deviation of 1.\n","     - **Robust Scaling**: Normalize using median and IQR to handle outliers.\n","   \n","   Provide the updated values after applying these techniques.\n","\n"]}]},{"cell_type":"markdown","source":["# Data normalization and scaling"],"metadata":{"id":"DKV9JYfgzeWN"}},{"cell_type":"code","source":["def generate_normalization_prompt(dataset_overview, target_columns=None, scaling_techniques=None):\n","    prompt = (\n","        \"You are an expert in data preprocessing and feature engineering. I need your help in normalizing and scaling \"\n","        \"data from a dataset used for machine learning.\\n\\n\"\n","        \"Dataset Context:\\n\"\n","        f\"{dataset_overview}\\n\\n\"\n","    )\n","\n","    if target_columns:\n","        prompt += (\n","            f\"Focus specifically on the following columns for normalization and scaling: {', '.join(target_columns)}.\\n\"\n","        )\n","    else:\n","        prompt += \"Analyze all numeric columns in the dataset for potential scaling.\\n\"\n","\n","    if scaling_techniques:\n","        prompt += (\n","            f\"Preferably suggest one or more of the following scaling techniques: {', '.join(scaling_techniques)}.\\n\"\n","        )\n","    else:\n","        prompt += (\n","            \"Recommend suitable scaling methods such as min-max scaling, z-score normalization, or log transformations \"\n","            \"based on the data characteristics.\\n\"\n","        )\n","\n","    prompt += (\n","        \"Include the following in your response:\\n\"\n","        \"- A summary of the dataset's scaling needs based on its metadata.\\n\"\n","        \"- Specific scaling or normalization methods for each column and why you suggest them.\\n\"\n","        \"- Any additional preprocessing steps needed to handle outliers or anomalies before scaling.\\n\\n\"\n","        \"If the dataset includes categorical columns or missing values, explain how to handle these issues.\\n\"\n","        \"Provide a concise yet detailed explanation of your recommendations.\"\n","    )\n","\n","    return prompt\n","\n","# Example usage:\n","dataset_overview = \"\"\"\n","The Titanic dataset includes the following columns:\n","- PassengerId (integer): Unique identifier for each passenger.\n","- Age (float): Passenger's age, with some missing values.\n","- Fare (float): Ticket price paid by the passenger.\n","- Embarked (categorical): Port of embarkation (C, Q, S).\n","- Survived (binary): Target column indicating survival (0 or 1).\n","\"\"\"\n","\n","prompt = generate_normalization_prompt(dataset_overview, target_columns=[\"Age\", \"Fare\"], scaling_techniques=[\"z-score\"])\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIz5RddXzXg4","executionInfo":{"status":"ok","timestamp":1733324464285,"user_tz":-60,"elapsed":504,"user":{"displayName":"Benoît Cayla","userId":"16127805405482571098"}},"outputId":"e69bef25-f749-43b8-fa21-820214c949a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["You are an expert in data preprocessing and feature engineering. I need your help in normalizing and scaling data from a dataset used for machine learning.\n","\n","Dataset Context:\n","\n","The Titanic dataset includes the following columns:\n","- PassengerId (integer): Unique identifier for each passenger.\n","- Age (float): Passenger's age, with some missing values.\n","- Fare (float): Ticket price paid by the passenger.\n","- Embarked (categorical): Port of embarkation (C, Q, S).\n","- Survived (binary): Target column indicating survival (0 or 1).\n","\n","\n","Focus specifically on the following columns for normalization and scaling: Age, Fare.\n","Preferably suggest one or more of the following scaling techniques: z-score.\n","Include the following in your response:\n","- A summary of the dataset's scaling needs based on its metadata.\n","- Specific scaling or normalization methods for each column and why you suggest them.\n","- Any additional preprocessing steps needed to handle outliers or anomalies before scaling.\n","\n","If the dataset includes categorical columns or missing values, explain how to handle these issues.\n","Provide a concise yet detailed explanation of your recommendations.\n"]}]},{"cell_type":"markdown","source":["# Anomaly detection and outlier treatment"],"metadata":{"id":"NZTsUW2yLKSI"}},{"cell_type":"code","source":["def build_anomaly_outlier_prompt(chunk, metadata):\n","    prompt = f\"\"\"\n","You are a data expert helping to clean a dataset by detecting and treating anomalies and outliers. The dataset is structured as follows:\n","Columns: {metadata['columns']}\n","Metadata about the columns:\n","- Missing value counts: {metadata['missing_counts']}\n","- Column means: {metadata['column_means']}\n","- Column medians: {metadata['column_medians']}\n","- Column modes: {metadata['column_modes']}\n","\n","Here is the dataset, including potential anomalies or outliers:\n","{chunk.head(5).to_string(index=False)}\n","\n","### Your task:\n","1. **Anomaly Detection**:\n","   - Identify any anomalies or outliers in the dataset. These may include:\n","     - Extreme values (e.g., values significantly larger or smaller than the rest).\n","     - Unexpected patterns in the data (e.g., missing values in key columns or unusual distributions).\n","     - Use techniques like z-scores, IQR (Interquartile Range), or visual inspection of statistical distributions to detect these anomalies.\n","\n","2. **Outlier Treatment**:\n","   - For each identified outlier or anomaly:\n","     - Determine if the value should be capped or transformed (e.g., winsorizing or log transformation).\n","     - Decide if the anomaly should be removed or replaced with a more plausible value (e.g., using the median or mean).\n","     - Ensure the final dataset is clean and consistent.\n","\n","Please provide the cleaned and updated values after applying these treatments.\n","\"\"\"\n","    return prompt\n","\n","prompt = build_anomaly_outlier_prompt(chunks[0], metadata)\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"OzNC0fWULMzG","executionInfo":{"status":"error","timestamp":1733324387844,"user_tz":-60,"elapsed":424,"user":{"displayName":"Benoît Cayla","userId":"16127805405482571098"}},"outputId":"cf55a235-6b07-497b-9f13-ada2bd077985"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'chunks' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c7af6d023fdd>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_anomaly_outlier_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'chunks' is not defined"]}]}]}